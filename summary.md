# WER Analysis: Whisper Base vs. Medium

## Dataset

- **Source:** [raianand/TIE_shorts](https://huggingface.co/datasets/raianand/TIE_shorts)
- **Split:** `test` (986 samples)
- **Domain:** Indian English academic lectures (NPTEL-style)
- **Distribution:** 928M / 58F | FAST 413, SLOW 374, AVG 199 | SOUTH 363, EAST 352, NORTH 202, WEST 69

## Methodology

### Task 1 — Whisper Base

- **Reference:** `Transcript` (as-is, no normalization)
- **Hypothesis:** `Normalised_Transcript` from dataset (pre-generated by Whisper Base)
- **Normalization (hypothesis only):** `ToLowerCase` → `RemovePunctuation` → `RemoveMultipleSpaces` → `Strip`
- No model inference — uses pre-existing transcriptions

### Task 2 — Whisper Medium

- **Model:** `openai/whisper-medium` (769M params), `language="en"`, CUDA
- **Reference:** `Transcript` (as-is, no normalization)
- **Hypothesis:** Live transcription from raw audio
- **Audio pipeline:** Extract → float32 mono → resample 16kHz → WAV → `model.transcribe()`
- **Normalization (hypothesis only):** Same as Task 1

Both tasks use `jiwer` for WER computation. Empty hypotheses are scored as WER = 1.0.

## Results

### Per-Model Summary

| Metric | Whisper Base | Whisper Medium |
|--------|:----------:|:------------:|
| Mean WER | 0.1882 (18.82%) | 0.2940 (29.40%) |
| Median WER | 0.1719 (17.19%) | 0.2600 (26.00%) |
| Std Dev | 0.1216 (12.16%) | 0.2129 (21.29%) |
| Min | 0.0000 (0.00%) | 0.0000 (0.00%) |
| Max | 1.0357 (103.57%) | 4.0000 (400.00%) |
| Perfect Scores | 40 | 3 |

### Head-to-Head

| Outcome | Samples | % |
|---------|:-------:|:-:|
| Base wins | 802 | 81.3% |
| Medium wins | 115 | 11.7% |
| Tied | 69 | 7.0% |

### Breakdown by Speech Class

| Class | n | Base WER | Medium WER | Diff |
|:-----:|:-:|:--------:|:----------:|:----:|
| FAST | 413 | 17.82% | 27.10% | +9.27 pp |
| AVG | 199 | 18.18% | 26.88% | +8.70 pp |
| SLOW | 374 | 20.26% | 33.30% | +13.03 pp |

### Breakdown by Region

| Region | n | Base WER | Medium WER | Diff |
|:------:|:-:|:--------:|:----------:|:----:|
| WEST | 69 | 16.78% | 27.39% | +10.62 pp |
| NORTH | 202 | 17.01% | 27.59% | +10.58 pp |
| SOUTH | 363 | 19.09% | 30.71% | +11.63 pp |
| EAST | 352 | 19.99% | 29.49% | +9.50 pp |

### Breakdown by Gender

| Gender | n | Base WER | Medium WER | Diff |
|:------:|:-:|:--------:|:----------:|:----:|
| Female | 58 | 20.70% | 27.68% | +6.99 pp |
| Male | 928 | 18.70% | 29.51% | +10.81 pp |

### Breakdown by Discipline (Medium)

| Discipline | n | Medium WER |
|:----------:|:-:|:----------:|
| Engineering | 691 | 29.73% |
| Non-Engineering | 295 | 28.63% |

## Top 20 High WER — Whisper Base

| # | ID | WER (%) | Speech Class | Region |
|:-:|:--:|:-------:|:------------:|:------:|
| 1 | 4XulH3TfbT0 | 103.57 | SLOW | EAST |
| 2 | zk1lXf7Ceiw | 100.00 | SLOW | SOUTH |
| 3 | RZQTTfU9TNA | 91.30 | FAST | SOUTH |
| 4 | F_cYtK9hP4o | 76.00 | SLOW | SOUTH |
| 5 | vlhnD_C2zWY | 75.00 | SLOW | EAST |
| 6 | -2aOCNaOiLs | 75.00 | SLOW | SOUTH |
| 7 | vlFdVYAXIxg | 73.91 | SLOW | EAST |
| 8 | bJom-GAZ2eU | 70.59 | FAST | EAST |
| 9 | vlImg6wCr8M | 67.86 | AVG | SOUTH |
| 10 | CPP29SU0cco | 66.67 | AVG | EAST |
| 11 | I5JVtCZVpm8 | 63.64 | FAST | SOUTH |
| 12 | nwcOtus_7eo | 61.90 | SLOW | NORTH |
| 13 | 4yCE67VwYJA | 57.89 | SLOW | EAST |
| 14 | coLQ8EfZTQI | 55.36 | SLOW | SOUTH |
| 15 | 8wlFZEVp7nw | 53.33 | FAST | NORTH |
| 16 | HQSgTo0Er5c | 51.02 | AVG | EAST |
| 17 | E-r2EDS0uP4 | 50.00 | SLOW | NORTH |
| 18 | jtMZfLViZu8 | 49.30 | SLOW | NORTH |
| 19 | UugCD-SaRFg | 48.89 | AVG | SOUTH |
| 20 | YrkbOeF7wFw | 48.44 | AVG | EAST |

14/20 are SLOW speech. EAST and SOUTH dominate. Most contain math notation or very short references.

## Top 20 High WER — Whisper Medium

| # | ID | WER (%) | Speech Class | Region |
|:-:|:--:|:-------:|:------------:|:------:|
| 1 | zk1lXf7Ceiw | 400.00 | SLOW | SOUTH |
| 2 | E-r2EDS0uP4 | 166.67 | SLOW | NORTH |
| 3 | vf0S_1ZITuA | 155.00 | SLOW | EAST |
| 4 | tsdoIka0mEg | 143.75 | SLOW | NORTH |
| 5 | 4XulH3TfbT0 | 135.71 | SLOW | EAST |
| 6 | 8PZx5kgLSqQ | 126.53 | SLOW | WEST |
| 7 | -2aOCNaOiLs | 125.00 | SLOW | SOUTH |
| 8 | Vpoi5W6a3lo | 116.67 | FAST | SOUTH |
| 9 | EayOCWaiRkw | 113.64 | SLOW | SOUTH |
| 10 | vlFdVYAXIxg | 102.17 | SLOW | EAST |
| 11 | qx86xQ6UKmU | 95.74 | FAST | SOUTH |
| 12 | kHeysMOk9h4 | 95.12 | SLOW | NORTH |
| 13 | lnD212vbPSk | 94.64 | FAST | SOUTH |
| 14 | RZQTTfU9TNA | 93.48 | FAST | SOUTH |
| 15 | qBw8ZQrJeys | 91.11 | SLOW | SOUTH |
| 16 | 3vVJkvTsbXQ | 88.57 | SLOW | NORTH |
| 17 | U6xmF1EqMEM | 88.10 | SLOW | EAST |
| 18 | vlImg6wCr8M | 87.50 | AVG | SOUTH |
| 19 | jtMZfLViZu8 | 81.69 | SLOW | NORTH |
| 20 | nwcOtus_7eo | 80.95 | SLOW | NORTH |

15/20 are SLOW speech. All male. 10/20 have WER > 100% (hallucinations).

Full data with reference text: [`results/top_20_high_wer.csv`](results/top_20_high_wer.csv)

## Why Certain Sentences Have High WER

- **Short references** — Samples with 1-3 words (e.g., "..") make WER explode from a single error
- **Math/symbolic content** — Equations like `ds/dt=pi r square` have no standard spoken form
- **Slow speech + pauses** — Whisper halluccinates filler text during silence, especially the medium model
- **Indian English accents** — Regional accents (SOUTH, EAST) diverge from Whisper's training data
- **Technical vocabulary** — Domain terms ("Kachchh", "eponymy") are out-of-vocabulary
- **Code-switching** — Hindi/regional words in English lectures confuse the English-only decoder
- **Asymmetric normalization** — Reference keeps punctuation/casing; hypothesis is stripped. This inflates WER artificially

## Why Base Outperforms Medium

1. **Hallucination** — Medium (769M params) generates confident but wrong text during pauses; Base (74M) is more conservative
2. **Stronger priors hurt** — Medium over-corrects accented speech to match its Western English training distribution
3. **SLOW speech gap** — Largest degradation (+13 pp), where medium hallucinates during long pauses
4. **Methodology caveat** — Task 1 uses pre-existing transcriptions; Task 2 does live inference, which may differ in post-processing

## Improving WER

**Normalization:** Normalize both reference and hypothesis equally; add number normalization; strip filler words ("uhhh", "you know")

**Model:** Fine-tune on Indian English data; try Whisper Large-v3; use `initial_prompt` for domain context; add VAD pre-processing to reduce hallucinations

**Data:** Filter short references (< 3 words) from evaluation; audit reference quality; report stratified WER (by region, speed, domain)

**Post-processing:** Language model rescoring; domain-specific spelling correction

## Reproducibility

| | Task 1 | Task 2 |
|-|--------|--------|
| Python | 3.10 | 3.10 |
| Env | `tie_wer_base` | `tie_wer_medium` |
| Packages | jiwer, datasets, pandas | jiwer, datasets, openai-whisper, torch, librosa |
| GPU | No | Yes |

```bash
# Task 1
cd task1_whisper_base && bash setup.sh
conda activate tie_wer_base && python wer_whisper_base.py

# Task 2
cd task2_whisper_medium && bash setup.sh
conda activate tie_wer_medium && python wer_whisper_medium.py
```
